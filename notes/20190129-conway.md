# Porting Conway

The Conway implementation is dominated by bit-munging code, which can simply be
glossed from C++ into Rust. It's nearly a search-and-replace job, since the two
languages' syntax are similar in this area.

And because it's a search and replace job, the result is probably slower than it
ought to be.

In particular: I wound up with a lot of this pattern:

    for x in 0 .. n {
        array[x] = foo;
        array[x + ] = bar;
    }

As written, that's bounds-checked. LLVM can eliminate a lot of those, but the
performance of this code isn't statically guaranteed the way it would be if we
were using iterators. For now, I've left it as a direct gloss of the C++, to
avoid introducing new bugs.

## Bitmap rasterizer redesign

This demo is also the point where I learned that the bitmap rasterizer API is
both harder and easier in Rust.

Recall that the C++ bitmap rasterizer is a class, inheriting from `Rasterizer`,
that owns foreground and background framebuffers and manages access to them.

That model already sounds fishy in Rust, because it implies that we're sharing a
single object between ISR and thread context. We can do this, of course, but
it's going to be awkward and/or error-prone. (I would argue that it's very much
error-prone to do so in C++.)

So, we want to split the rasterizer between ISR and thread components. At the
very least, each end needs to have a framebuffer, and on frame change they swap
framebuffers.

You could imagine phrasing this as a synchronous rendezvous, in which the thread
and ISR exchange buffers, but that's the wrong way to approach it. If the thread
isn't ready, we can't block the ISR, or video will distort. And if the thread
isn't ready, even *checking to see if it's ready* would add unnecessary code to
the ISR in a context where we're already CPU-starved.

The lowest overhead option is to handle the swap from thread code, doing it
during vblank to minimize risk of glitching.

I tried to implement this using atomic pointer swaps and spinlocks and the like,
but it turned out that a new primitive was needed. Here's why:

    let fg = SpinLock::new( /* reference to one buffer */ );
    let mut bg = /* reference to the other buffer */;

    vga::take_hardware()
        ...
        .with_raster(
            |ln, tgt, ctx| {
                // Lock the foreground, panicking if the thread contends.
                let fg = fg.try_lock().unwrap();
                // Rasterize it here.
            },
            |vga| loop {
                // Try to avoid glitching the ISR
                vga.sync_to_vblank();
                // Lock the spinlock long enough to swap the buffers.
                core::mem::swap(&mut bg, &mut *fg.lock());

                // Okay, now we generate the next frame. Conway uses the
                // previous frame as input, so we need to...
                conway::step(fg....?, bg);
                // crap.
            },
        )

The design of the algorithm explicitly requires that the ISR and thread read
from the current foreground framebuffer *simultaneously*, and the thread write
exclusively to the background. Because the thread retains a reference to the
buffer handed to the ISR, neither a SpinLock nor a simple exchange are correct.

So I implemented a `ReadWriteLock`.

This *technically* adds overhead to the ISR, the way I said I wanted to avoid.
It locks the `ReadWriteLock` to check whether it's clashing with the thread
buffer swap operation. I could probably optimize this later. In practice it adds
around 20-30 cycles.

(Okay, with a bit of optimization: 13 cycles. One optimization relies on twos'
complement integer representation, which means it would be unsound in C++.)


I'm reasonably happy with the state of this code, given how simple it is
relative to the original.

---

And then there's the part where it panics. It actually panics *statically* -- if
you read the disassembly right now, the code straight-lines into a panic
handler. (I'm disappointed the compiler didn't report this, actually, but I'm
not aware of any that do.)

The panic message, reported through ITM, reads:

    panicked at 'index out of bounds: the len is 15000 but the index is
      2341630177', src/bin/conway/conway.rs:124:29

I'm delighted about this. Why am I delighted? Because this is an error I would
not have gotten in C++ given the exact same code. Let's fix it, and then see if
it's present in the original.

Without even looking at the code I can tell one thing: that 15000 is
`800*600/32`, which means we were referencing one of the framebuffers.

Line 124 is here:

    for x in 0 .. (WIDTH - 1) {
        adv(&mut above, current_map[offset - WIDTH + x + 1]);
        adv(&mut current, current_map[offset + x + 1]);
    124 adv(&mut below, current_map[offset + WIDTH + x + 1]);
        next_map[offset + x] = col_step(&above, &current, &below);
    }

So the expression `current_map[offset + WIDTH + x + 1]` is out of bounds. And
way the hell out of bounds, too -- look at that giant index, 2341630177.

The index's hex value, 0x8B9270E1, is not meaningful to me.

(Note from the future: this turned out to be a red herring, GDB was starting the
binary with the wrong stack and it was getting stomped.)

Looks like the value of `WIDTH` is wrong. But I'm also seeing ISR data races;
apparently the PendSV copy-scan-buffer routine is running into SAV.

That strikes me as odd. I didn't remember it taking up that much of hblank. I
notice that I explicitly placed it in RAM in C++ and it's in Flash here -- I
wonder how much of a difference that makes?

Currently it's taking 3.25 us to copy 200 words, or 2.6 cycles per word. That's
pretty close to theoretical max.

Moving `copy_words_impl` into RAM makes no difference, but moving the whole
PendSV handler into RAM -- avoiding a bounce through a cold Flash thunk on the
way to `copy_words_impl` -- reduces it to 3.106 us, or 2.48 cycles/word.
Okaaaaaay. This is just fast enough to avoid the race, which is not a great way
to avoid races.

Heh. Turns out I can rearrange the order of actions in the PendSV handler and
front-load all the potentially racing operations. That's nice.

Okay! That race gone, I'm hitting the index-out-of-bounds error reliably.


About that index-out-of-bounds error. I noted that WIDTH looked wrong, and
fixing it stops the error.

Looks like things are working.

----

Reconstructing my changes above more rigorously...

Swapping the order of operations in PendSV has us performing the racy operations
on `NEXT_XFER` within the first microsecond of the hblank. The `copy_words`
kicks in just after and consumes the rest of the hblank, overlapping with the
SAV ISR. It's still running when we start DMAing pixels out, which has the
potential for AHB conflicts and jitter.

Moving just `copy_words` into RAM improves things veeeerry slightly but doesn't
fix the problem.

Moving the entire PendSV interrupt, and all its flattened inlined code, into
RAM? Fixes it. `copy_words` completes with 380ns to spare before the shock
absorber kicks in to prepare for SAV.

That's not a *great* margin. 60 cycles. But it'll do for now.

How about moving the very important latency-sensitive horizontal retrace ISRs
into RAM? In Flash EAV takes 792ns and SAV, 558ns. In RAM, SAV drops to 512ns.
Meh? It's probably important to keep this to reduce jitter, but it's not a huge
savings.

---

Bitmap rasterization in Conway takes 6.04us. The rasterization callback itself
is a closure; it isn't obvious how to place a closure's code in a particular
section. But the bulk of it will be spent in the unpacker, which is not
currently in RAM.

Placing it in RAM forces control to go through a cold Flash thunk. Laaaame, I
wonder if I can adjust this behavior. Anyhoo. In RAM? 6.08us.

So, no improvement. I had asserted on my blog years ago that moving routines
into RAM doesn't improve much with the flash accelerator turned on, *unless* you
need predictable start latency... looks like I may have been right.

---

Conway takes 13.78ms to render a frame, leaving 3.1ms idle. Not bad. Time to
look at the code.


